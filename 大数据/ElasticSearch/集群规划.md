# Elasticsearch 集群规划

## 专用 Master 节点

+ 只提供集群管理功能,不提供索引和保存数据功能
+ 对内存、CPU的需求非常小;
+ 不受 JVM GC影响,服务更稳定;
+ 至少3个节点,保证高可用。

## Hot-Warm节点

+ 背景:

  + 索引速度和聚合查询速度矛盾:
    + 为了加快索引速度,需要将负载分散到尽可能多的 Shards 和 Data Nodes 中;
    + 为了减少查询延迟, Shard 越少越好,但是单 Shard 又不能太大(<50GB, doc上限 2^31), 否则查询时间较长,而且reallocating/recovering 的时候时间太长;
    + 为 index 定义多少 Shard 合适;

  + 新旧数据使用频率和存储需求有差别:
    + 新数据: 需要快速地索引和存储,需要进可能短的延迟和较大吞吐量, SSD磁盘的机器读能力较强;
    + 老数据: 需要长时间保存,占用的磁盘空间大,较少被查询,所以可以保存到大容量 SATA 磁盘机器上;
    + 使用频率: 新数据被用到的概率比较大(如前端 dashboard 查询), 老数据很少被用到。
    + 什么时候老化 SSD 上的新数据,并移动到 Sata 机器？
      + 索引的文档数量;
      + 索引占据的磁盘空间;
      + 索引创建时长;
      + 使用 Index Rollover API实现 index 的老化;
      + Rollover 相对于按照日期创建 index 的优势是: 解决以日期作为索引名称时, 索引大小不均衡的问题;
  
  + 索引和查询数据时,如何屏蔽 SSD 和 Sata 混合 Data 节点的差别？
      + 定义两个index alias: active-xxx, search-xxx;
      + active-xxx: 包含新数据的 index 列表,用于索引当前数据;
      + search-xxx: 包含新、老数据的 index 列表, 用于搜索数据;

  + 使用 Curator 工具将 Index Rollover、Index 移动、force merge等过程自动化;

  + Hot Data 节点
    + 本地 SSD 盘，容量小，提供短期数据存储能力和高性能 IO 读写能力；
    + 索引和保存当前的数据；
    + 只保存最近 1 周的数据，用于 dashboard 即时查询；
    + 至少 3 个节点，保证高可用；
    + 性能水平线性伸缩；

+ Warm Data 节点：
    + 本地大容量 SATA 盘，提供大容量和长时间存储能力；
    + 保存 1 周以前的历史只读数据，最长保留 1 个月；
    + 设置 index.codec: best_compression 参数，压缩历史数据，减少占用的磁盘空间；
    + 至少 3 个节点，保证高可用；
    + 性能水平线性伸缩；

  + 实现方案：
    + SSD 和 Sata Data Node 分别配置 node.attr.box_type 参数为 Hot 或 Warm；
    + index template 配置：
        + 参数 "index.routing.allocation.require.box_type" 为 "Hot"，将 shard 分配到 Hot 节点；
        + 配置合适的 primary shard 数量，每个 shard 不要超过 50GB；
        + 2 副本，或关闭副本功能；
        + 只 analysis 必要的字段；
        + 关闭 _all 和 _source；
    + 1 周后，配置 index 的参数 "index.routing.allocation.require.box_type" 为 warm，将 index 数据移动到 Warm 节点的目的；
    + 对移动到 warm 节点的 read-only index 执行 fore merge 操作：
        + 减少 index segment 文件数量，从而减少占用的磁盘空间和内存；
        + 提高查询性能；
        + 注意：force merge 期间会消耗较大的系统资源(CPU、内存、磁盘)，故不适合在索引当前数据的 Hot 节点上操作，同时不适合对当前还在写数据的 index 进行操作；
    + 可以使用 Curator 工具将 index 移动、force merge 的过程自动化；

## 使用 JBOD 多磁盘还是做了 RAID 的逻辑盘，数据安全性&IO性能的抉择

    + JBOD 多磁盘：path.data 列出多块磁盘，ES 做 shard 级别的 strip，各 shard 分别保存到不同磁盘上，单 shard 不会跨磁盘；
    + 做了 RAID0 的逻辑盘：RAID0 卡做 data block 级别的 strip，一个 shard 的数据可能分配到多块磁盘上；
    + JBOD 数据丢失风险更小：
    + 如果磁盘故障，只影响那一块磁盘上的 Shard 数据；
    + 对于做了 RAID0 的逻辑盘，如果一块磁盘故障，整个机器的数据丢失不可用；数据丢失风险随着磁盘数目成倍增长；
    + 做了 RAID0 的逻辑盘：因为 RAID 卡有缓存、异步写等机制，所以 IO 读写性能更高，大概是单盘的好几倍；

    抉择：

    + Hot Data 节点对磁盘 IO 性能要求比较高，由于是多副本且容量稍小，故单块磁盘故障后可以快速恢复且不影响整体可用性，故使用 RAID0 方案；
    + Warm Data 节点，数据盘较多，存储容量较大，单盘故障时不应该影响其它盘的数据可用性，使用 JBOD 方案；

    参考：
    https://discuss.elastic.co/t/raid-0-ssd/46504/5?u=jun_zhang
    https://discuss.elastic.co/t/raid-0-ssd/46504/7?u=jun_zhang
    https://www.elastic.co/blog/hot-warm-architecture
    https://discuss.elastic.co/t/to-multi-path-data-or-hardware-raid-or-not/40143